{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to accomplish dataset foundation together. The two datasets would require same labels, and shuffle in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "# CSV reading/writing\n",
    "import pandas as pd\n",
    "import csv\n",
    "import md5_batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NVSS',\n",
       " 'Dataset_download_testing.ipynb',\n",
       " 'FRDEEPv2_tutorial.ipynb',\n",
       " '.DS_Store',\n",
       " 'md5_batch_gen.py',\n",
       " 'Step2_FRDEEP_Astroquery_Download.ipynb',\n",
       " '3_Source_preprocessed_png_image',\n",
       " '4_DataPickle_Generation',\n",
       " 'FIRST',\n",
       " 'object_id',\n",
       " 'Step4_Create_FRDEEPv2.ipynb',\n",
       " 'NVSS_IMG',\n",
       " 'Dataset_evaluation.ipynb',\n",
       " '__pycache__',\n",
       " 'Step5_FRDEEPv2_loading_test.ipynb',\n",
       " 'FIRST.tar.gz',\n",
       " '1_Source_catalogue_spreadsheets',\n",
       " 'FRDEEPv2_foundation.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'FRDEEPv2.tar.gz',\n",
       " 'Step3_FRDEEP_FITS_PNG_conversion.ipynb',\n",
       " 'NVSS.tar.gz',\n",
       " 'FIRST_IMG',\n",
       " '2_Source_raw_fits_image']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Functions necessary to deal with the dataset foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(datadir,file_format):\n",
    "    \"\"\"\n",
    "        This function returns file names in selected format under given directory.\n",
    "\n",
    "        Args:\n",
    "        datadir: the directory where the selected format files are saved.\n",
    "        file_format: customized file format waiting for seaching.\n",
    "\n",
    "        Returns:\n",
    "        file_list: a list of file names under the given directory.\n",
    "        \n",
    "    \"\"\"\n",
    "    # get list of FITS files:\n",
    "    file_list = []\n",
    "    # Iterate files over directory\n",
    "    for filename in os.listdir(datadir):\n",
    "        if filename.endswith(\".\"+ file_format):\n",
    "            name = os.path.join(filename)\n",
    "            file_list.append(name)\n",
    "    print('Number of file under the directory: ',len(file_list))\n",
    "    return file_list\n",
    "\n",
    "def make_dir(dir_to_make):\n",
    "    \"Make directory if it doesn't exist\"\n",
    "    if not os.path.isdir(dir_to_make):\n",
    "        os.mkdir(dir_to_make)\n",
    "        \n",
    "def image_max_value(image_file,file_dir):\n",
    "    \"check if the selected image has non zero maximum value\"\n",
    "    img_max = 0\n",
    "    with Image.open(file_dir+image_file) as img:\n",
    "        img = np.asarray(img)\n",
    "        if img.max() > 0:\n",
    "            img_max = 1\n",
    "        else:\n",
    "            print(f\"Image {image_file} is black.\\n\")\n",
    "    return img_max\n",
    "\n",
    "# ------------------------------------\n",
    "# Index randomisation\n",
    "def randomise_by_index(inputlist,idx_list):\n",
    "\n",
    "    \"\"\"\n",
    "       Function to randomize an array of data\n",
    "    \"\"\"\n",
    "    if len(inputlist)!=len(idx_list):\n",
    "        print(\"These aren't the same length\")\n",
    "    outputlist = []\n",
    "    for element in idx_list:\n",
    "        outputlist.append(inputlist[element])\n",
    "\n",
    "    return outputlist\n",
    "\n",
    "# ------------------------------------\n",
    "# Image normalization\n",
    "def Image_normalize(restore_dir,filename):\n",
    "    im = Image.open(restore_dir+filename)\n",
    "    im = (np.array(im))\n",
    "    # Normalize to [0,255]\n",
    "    img_max, img_min = im.max(),im.min()\n",
    "    if img_max != 0:\n",
    "        im = (im - img_min) / (img_max - img_min)\n",
    "        im *= 255.\n",
    "        \n",
    "    gray = im[:,:].flatten()\n",
    "    filedata = np.array(list(gray),np.uint8)\n",
    "    return filedata    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directories of **NVSS/FIRST** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVSS/FIRST image folder dir\n",
    "NVSS_dir = './3_Source_preprocessed_png_image/NVSS/'\n",
    "FIRST_dir = './3_Source_preprocessed_png_image/FIRST/'\n",
    "# Saving directory\n",
    "save_dir = './4_DataPickle_Generation/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file name under each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of file under the directory:  658\n",
      "Number of file under the directory:  658\n"
     ]
    }
   ],
   "source": [
    "NVSS_file_list = get_file_list(NVSS_dir,'png')\n",
    "FIRST_file_list = get_file_list(FIRST_dir,'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split FR I/II according to their labels [NVSS and FIRST are the same]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# NVSS object file names\n",
    "FRI_list = [element for element in NVSS_file_list if element[:-4].split('_')[1] == 'I']\n",
    "FRII_list = [element for element in NVSS_file_list if element[:-4].split('_')[1] == 'II']\n",
    "# Find the longest object id\n",
    "longest_id_FRI = max(FRI_list, key=len)\n",
    "longest_id_FRII = max(FRII_list, key=len)\n",
    "max_id_len = len(max(longest_id_FRI,longest_id_FRII)) + 1 # + 1 is simply just in cases.\n",
    "print(max_id_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory to save **FRDEEP v2** dataset pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory definition\n",
    "FIRST_save_dir = save_dir + \"FIRST/\" # FIRST image pickle\n",
    "NVSS_save_dir = save_dir + \"NVSS/\" # NVSS image pickle\n",
    "id_save_dir = save_dir + \"object_id/\" # object id pickle\n",
    "# Make directory if not exist\n",
    "make_dir(FIRST_save_dir)\n",
    "make_dir(NVSS_save_dir)\n",
    "make_dir(id_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.7\n",
    "\n",
    "# label names:\n",
    "label_names = ['FR I','FR II']\n",
    "\n",
    "# length of data arrays [npix x npix x rgb] \n",
    "nvis = 150 * 150 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FR Is\n",
    "FRI_train = FRI_list[:int(split_ratio * len(FRI_list))]\n",
    "FRI_test = FRI_list[int(split_ratio * len(FRI_list)):]\n",
    "# FR IIs\n",
    "FRII_train = FRII_list[:int(split_ratio * len(FRII_list))]\n",
    "FRII_test = FRII_list[int(split_ratio * len(FRII_list)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loop through and fill the batches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. dataset file/folder generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  50%|#####     | 1/2 [00:05<00:05,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of the current batch: 460\n",
      "3C 207                          3C 207_II.png 3C 207_II.png 1\n",
      "3C 240                          3C 240_II.png 3C 240_II.png 1\n",
      "SDSS J121519.19+472142.4        SDSS J121519.19+472142.4_I.png SDSS J121519.19+472142.4_I.png 0\n",
      "TXS 1409-030                    TXS 1409-030_II.png TXS 1409-030_II.png 1\n",
      "SDSS J134745.19+503203.5        SDSS J134745.19+503203.5_I.png SDSS J134745.19+503203.5_I.png 0\n",
      "# of source object id saved in this batch:  460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|##########| 2/2 [00:07<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of the current batch: 198\n",
      "3C 334                          3C 334_II.png 3C 334_II.png 1\n",
      "4C 10.40                        4C 10.40_II.png 4C 10.40_II.png 1\n",
      "SDSS J150148.14+163345.6        SDSS J150148.14+163345.6_I.png SDSS J150148.14+163345.6_I.png 0\n",
      "TXS 1536+144                    TXS 1536+144_II.png TXS 1536+144_II.png 1\n",
      "BWE 1459+2451                   BWE 1459+2451_II.png BWE 1459+2451_II.png 1\n",
      "# of source object id saved in this batch:  198\n",
      "Dataset foundation finished:)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_FIRST_name,full_NVSS_name = [], []\n",
    "# ------------------------------------\n",
    "# loop through and fill the batches:\n",
    "for batch in tqdm(range(2), ascii=True, desc='Batch'):\n",
    "    \n",
    "    if (batch==1):\n",
    "        # the last batch is the test batch:\n",
    "        oname = \"test_batch\"\n",
    "        batch_label = 'testing batch 1 of 1'\n",
    "        FR1_list = FRI_test\n",
    "        FR2_list = FRII_test\n",
    "    else:\n",
    "        # everything else is a training batch:\n",
    "        oname = \"train_batch\"\n",
    "        batch_label = 'training batch 1 of 1'\n",
    "        FR1_list = FRI_train\n",
    "        FR2_list = FRII_train\n",
    "    # create empty arrays for the batches:\n",
    "    labels=[]\n",
    "    filedata_FIRST=[];data_FIRST=[];filenames_FIRST=[]\n",
    "    filedata_NVSS=[];data_NVSS=[];filenames_NVSS=[]\n",
    "    file_id = []\n",
    "    \n",
    "    # get FRI radio galaxies:\n",
    "    for i in range(len(FR1_list)):\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # FIRST data\n",
    "        # Get file names in the specific batch\n",
    "        filename_FIRST = FR1_list[i]\n",
    "        # Save these filenames in a list in sequence.\n",
    "        filenames_FIRST.append(filename_FIRST)\n",
    "        full_FIRST_name.append(filename_FIRST)\n",
    "        # Normalize the image to [0,255]\n",
    "        filedata_FIRST = Image_normalize(FIRST_dir,filename_FIRST)\n",
    "        # Save normalized image data to data list.\n",
    "        data_FIRST.append(filedata_FIRST)\n",
    "        # ------------------------------------------\n",
    "        # NVSS data\n",
    "        # Get file names in the specific batch\n",
    "        filename_NVSS = FR1_list[i] # the filename of the same image (NVSS vs. FIRST)is the same.\n",
    "        # Save these filenames in a list in sequence.\n",
    "        filenames_NVSS.append(filename_NVSS)\n",
    "        full_NVSS_name.append(filename_NVSS)\n",
    "        # Normalize the image to [0,255]\n",
    "        filedata_NVSS = Image_normalize(NVSS_dir,filename_NVSS)\n",
    "        # Save normalized image data to data list.\n",
    "        data_NVSS.append(filedata_NVSS)\n",
    "        # ------------------------------------------\n",
    "        # Source object id\n",
    "        file_id.append(\"{:<31}\".format(FR1_list[i][:-4].split('_')[0]))\n",
    "        # ------------------------------------------\n",
    "        # Label them as 0\n",
    "        labels.append(0)\n",
    "\n",
    "    # get GRG radio galaxies:\n",
    "    for i in range(len(FR2_list)):\n",
    "        # ------------------------------------------\n",
    "        # FIRST data\n",
    "        # Get file names in the specific batch\n",
    "        filename_FIRST = FR2_list[i]\n",
    "        # Save these filenames in a list in sequence.            \n",
    "        filenames_FIRST.append(filename_FIRST)\n",
    "        full_FIRST_name.append(filename_FIRST)\n",
    "        # Normalize the image to [0,255]\n",
    "        filedata_FIRST = Image_normalize(FIRST_dir,filename_FIRST)\n",
    "        # Save normalized image data to data list.\n",
    "        data_FIRST.append(filedata_FIRST)\n",
    "        # ------------------------------------------\n",
    "        # NVSS data\n",
    "        # Get file names in the specific batch\n",
    "        filename_NVSS = FR2_list[i]\n",
    "        # Save these filenames in a list in sequence.\n",
    "        filenames_NVSS.append(filename_NVSS)\n",
    "        full_NVSS_name.append(filename_NVSS)\n",
    "        # Normalize the image to [0,255]\n",
    "        filedata_NVSS = Image_normalize(NVSS_dir,filename_NVSS)\n",
    "        # Save normalized image data to data list.\n",
    "        data_NVSS.append(filedata_NVSS)\n",
    "        # ------------------------------------------\n",
    "        # Source object id\n",
    "        file_id.append(\"{:<31}\".format(FR2_list[i][:-4].split('_')[0]))\n",
    "        # ------------------------------------------\n",
    "        # Label them as 1\n",
    "        labels.append(1) \n",
    "    print(f'len of the current batch: {len(file_id)}')\n",
    "    # randomise data in batch: (the truth is, it does not shuffle the data)\n",
    "    idx_list = [i for i in range(0,len(file_id))]\n",
    "    shuffle(idx_list)\n",
    "    labels = randomise_by_index(labels,idx_list)\n",
    "    # FIRST\n",
    "    data_FIRST = randomise_by_index(data_FIRST,idx_list)\n",
    "    filenames_FIRST = randomise_by_index(filenames_FIRST,idx_list)\n",
    "    # NVSS\n",
    "    data_NVSS = randomise_by_index(data_NVSS,idx_list)\n",
    "    filenames_NVSS = randomise_by_index(filenames_NVSS,idx_list)\n",
    "    # Source object id\n",
    "    file_id = randomise_by_index(file_id,idx_list)\n",
    "    \n",
    "    # Print testing data names\n",
    "    for jj in range(5):\n",
    "        print(file_id[jj],filenames_FIRST[jj],filenames_NVSS[jj],labels[jj])\n",
    "            \n",
    "    # create dictionary of FIRST batch:\n",
    "    dict_FIRST = {\n",
    "            'batch_label':batch_label,\n",
    "            'labels':labels,\n",
    "            'data':data_FIRST,\n",
    "            'filenames':filenames_FIRST\n",
    "            }\n",
    "    dict_NVSS = {\n",
    "            'batch_label':batch_label,\n",
    "            'labels':labels,\n",
    "            'data':data_NVSS,\n",
    "            'filenames':filenames_NVSS\n",
    "            }\n",
    "    # write pickled output for FRDEEP-F:\n",
    "    with io.open(FIRST_save_dir+oname, 'wb') as f:\n",
    "        pickle.dump(dict_FIRST, f)\n",
    "    # write pickled output for FRDEEP-N:\n",
    "    with io.open(NVSS_save_dir+oname, 'wb') as f:\n",
    "        pickle.dump(dict_NVSS, f)\n",
    "    # save object id\n",
    "    print('# of source object id saved in this batch: ',len(file_id))\n",
    "    file_id = np.asarray(file_id)\n",
    "    np.save(id_save_dir+oname,file_id)\n",
    "# end batch loop\n",
    "# ------------------------------------\n",
    "# now write the meta data file:\n",
    "oname = 'batches.meta'\n",
    "\n",
    "# ------------------------------------\n",
    "# create dictionary of batch (FIRST):\n",
    "dict_FIRST = {\n",
    "        'label_names':label_names,\n",
    "        'num_vis':nvis,\n",
    "        }\n",
    "# create dictionary of batch:\n",
    "dict_NVSS = {\n",
    "        'label_names':label_names,\n",
    "        'num_vis':nvis,\n",
    "        }\n",
    "\n",
    "# ------------------------------------\n",
    "# write pickled output (FRDEEP-F):\n",
    "with io.open(FIRST_save_dir+oname, 'wb') as f:\n",
    "    pickle.dump(dict_FIRST, f)\n",
    "# write pickled output (FRDEEP-N):\n",
    "with io.open(NVSS_save_dir+oname, 'wb') as f:\n",
    "    pickle.dump(dict_NVSS, f)    \n",
    "\n",
    "print('Dataset foundation finished:)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  tar gz files for selected folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    \"https://stackoverflow.com/questions/2032403/how-to-create-full-compressed-tar-file-using-python\"\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define output file names/ source directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./4_DataPickle_Generation/FIRST.tar.gz\n",
      "./4_DataPickle_Generation/NVSS.tar.gz\n",
      "./FRDEEPv2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# FIRST \n",
    "output_filename_FIRST = FIRST_save_dir[:-1] + '.tar.gz'\n",
    "source_dir_FIRST = FIRST_save_dir\n",
    "make_tarfile(output_filename_FIRST, source_dir_FIRST)\n",
    "print(output_filename_FIRST)\n",
    "# NVSS\n",
    "output_filename_NVSS = NVSS_save_dir[:-1] + '.tar.gz'\n",
    "source_dir_NVSS = NVSS_save_dir\n",
    "make_tarfile(output_filename_NVSS, source_dir_NVSS)\n",
    "print(output_filename_NVSS)\n",
    "# Save the whole thing as a compressed file\n",
    "output_filename_FRDEEPv2 = './FRDEEPv2.tar.gz'\n",
    "make_tarfile(output_filename_FRDEEPv2, save_dir)\n",
    "print(output_filename_FRDEEPv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Copy image files to the new upper directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directories to copy the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVSS_dir_target = save_dir + 'NVSS_IMG/'\n",
    "FIRST_dir_target = save_dir + 'FIRST_IMG/'\n",
    "# make directory (if not exist)\n",
    "make_dir(NVSS_dir_target)\n",
    "make_dir(FIRST_dir_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image directories to be copied to (origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_file_dir_origin = [FIRST_dir + i for i in full_FIRST_name]\n",
    "NVSS_file_dir_origin = [NVSS_dir + i for i in full_NVSS_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image directories to get copied (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_file_dir_target = [FIRST_dir_target + i for i in full_FIRST_name]\n",
    "NVSS_file_dir_target = [NVSS_dir_target + i for i in full_NVSS_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for kk in range(len(FIRST_file_dir_origin)):\n",
    "    shutil.copyfile(FIRST_file_dir_origin[kk], FIRST_file_dir_target[kk]) # FIRST image copy\n",
    "    shutil.copyfile(NVSS_file_dir_origin[kk], NVSS_file_dir_target[kk]) # FIRST image copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate md5 code for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'a8bb67d1caf2d0ca9fa501b337e39ea6'] [['train_batch', 'a7bd9f5f3f27f395f51e424a89e48db9']] ['test_batch', '9a46e93a9932f986efc94fddc7de0164']\n",
      "3b6632b869370e0c678ab69dfe43d4c5\n"
     ]
    }
   ],
   "source": [
    "meta_NVSS, data_batches_NVSS, test_batch_NVSS = md5_batch_gen.md5_data_batch_gen(NVSS_save_dir[:-1])\n",
    "print(meta_NVSS, data_batches_NVSS, test_batch_NVSS)\n",
    "NVSS_tgz_md5 = md5_batch_gen.md5_gen(output_filename_NVSS)\n",
    "print(NVSS_tgz_md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'a8bb67d1caf2d0ca9fa501b337e39ea6'] [['train_batch', '234b66460e95834c78cee7c7a73ed916']] ['test_batch', 'acb0277e9feb983ff8fa79c31c4b395a']\n",
      "c57d884b8daba9aa5bbce8f383594291\n"
     ]
    }
   ],
   "source": [
    "meta_FIRST, data_batches_FIRST, test_batch_FIRST = md5_batch_gen.md5_data_batch_gen(FIRST_save_dir[:-1])\n",
    "print(meta_FIRST, data_batches_FIRST, test_batch_FIRST)\n",
    "FIRST_tgz_md5 = md5_batch_gen.md5_gen(output_filename_FIRST)\n",
    "print(FIRST_tgz_md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45bad6ed4e96ee4685306c013b0953f9\n"
     ]
    }
   ],
   "source": [
    "FULL_tgz_md5 = md5_batch_gen.md5_gen(output_filename_FRDEEPv2)\n",
    "print(FULL_tgz_md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
